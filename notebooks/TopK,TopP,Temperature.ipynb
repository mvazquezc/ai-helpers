{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Top_K, Top_P, and temperature for LLMs\n",
    "Before trying this Notebook, read the blog post [here](https://linuxera.org/turning-the-knobs-of-llm-text-generation/)\n",
    "\n",
    "## vLLM\n",
    "\n",
    "We will be using vLLM python bindings, the parameters can be set via the [SamplingParams](https://docs.vllm.ai/en/stable/api/vllm/#vllm.SamplingParams) class. Below the descriptions for each parameter.\n",
    "\n",
    "- [temperature](https://docs.vllm.ai/en/stable/api/vllm/#vllm.SamplingParams.temperature) – Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling. (0, 1)\n",
    "\n",
    "- [top_p](https://docs.vllm.ai/en/stable/api/vllm/#vllm.SamplingParams.top_p) – Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1). Set to 1 to consider all tokens.\n",
    "\n",
    "- [top_k](https://docs.vllm.ai/en/stable/api/vllm/#vllm.SamplingParams.top_k) – Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\u001b[0m\n",
      "Using CPython 3.12.12 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n",
      "Creating virtual environment with seed packages at: \u001b[36m.venv\u001b[39m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==25.3\u001b[0m\n",
      "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m152 packages\u001b[0m \u001b[2min 5.73s\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m50 packages\u001b[0m \u001b[2min 13.91s\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m7 packages\u001b[0m \u001b[2min 113ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m50 packages\u001b[0m \u001b[2min 245ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manthropic\u001b[0m\u001b[2m==0.71.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mapache-tvm-ffi\u001b[0m\u001b[2m==0.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcbor2\u001b[0m\u001b[2m==5.7.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.12.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdnspython\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1memail-validator\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cloud-cli\u001b[0m\u001b[2m==0.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastar\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mflashinfer-python\u001b[0m\u001b[2m==0.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjmespath\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.8.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmodel-hosting-container-standards\u001b[0m\u001b[2m==0.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mninja\u001b[0m\u001b[2m==1.13.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-frontend\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cutlass-dsl\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai-harmony\u001b[0m\u001b[2m==0.0.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.2.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpybase64\u001b[0m\u001b[2m==1.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-extra-types\u001b[0m\u001b[2m==2.10.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mray\u001b[0m\u001b[2m==2.52.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrignore\u001b[0m\u001b[2m==0.7.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetproctitle\u001b[0m\u001b[2m==1.3.7\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==75.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.11.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.33.post1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.25\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv venv --python 3.12 --seed --clear\n",
    "!source .venv/bin/activate\n",
    "!uv pip install vllm --torch-backend=auto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-25 16:45:08 [utils.py:253] non-default args: {'max_logprobs': -1, 'disable_log_stats': True, 'model': 'facebook/opt-125m'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b9075b200e43d3b9809820aa62c558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-25 16:45:29 [model.py:631] Resolved architecture: OPTForCausalLM\n",
      "INFO 11-25 16:45:29 [model.py:1745] Using max model len 2048\n",
      "INFO 11-25 16:45:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a47a84eb00c44ec9e818924e4b8584d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3ec3701df34bcba2bc39bd9fdf2617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729554887ded4d7b85b0ba48f6254201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30054a64a36f4cc7922e3a00b445b41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491663654d7742c78d828bee8fd6d7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-25 16:45:35 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 11-25 16:48:58 [llm.py:352] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# max_logprobs=-1 will help us see token distribution/cumulative probabilities later\n",
    "llm = LLM(model=\"facebook/opt-125m\", max_logprobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top_K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Temperature 0, top_k 10\n",
    "\n",
    "In this test we will configure top_k to return the top 10 tokens with the highest probability, on top of that we are configuring a temperature of 0, this means the model will always use the token with the highest probability. We will run 5 generations and we will see we always get the same output.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2de0c21bb2643c88e887d451ba36c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f7a9c6d1fc47d38da2b8ffa54f9f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n",
      "Generation 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3160264064a24fff98ac28b70779e51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb4f6c08bd74241b82f3dad75e609b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n",
      "Generation 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ecefe184354defa0b89c359e884f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2658d30a94401fbeb52ed72cd5f955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n",
      "Generation 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc657b4c05cb43c685cb0d2d4cc3a811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b583137a2c834638818bc328f81fdd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0, top_k=10)\n",
    "\n",
    "for i in range(0,4):\n",
    "  print(f\"Generation {i+1}\")\n",
    "  outputs = llm.generate(prompts, sampling_params)\n",
    "  for output in outputs:\n",
    "      prompt = output.prompt\n",
    "      generated_text = output.outputs[0].text\n",
    "      print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Temperature 0.9, top_k 10\n",
    "\n",
    "In this test we will configure top_k to return the top 10 tokens with the highest probability, but this time, temperature is 0.9, this should led to different generations. The model will use on of the top 10 tokens in a less predictable way. We will run 5 generations and we will likely see different outputs for each generation.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517ee88889c1459cb4e848af66e8a085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acba15aba86243a68a75cafd526a2550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of Europe. The people of France and Europe are the most beautiful countries'\n",
      "Generation 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d48c7e0cb54d719e59995cc6a89dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1e4061a3684b8a9c61591d3563536f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the European Union with a population of about 10.6 million people'\n",
      "Generation 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c81828c55846889422aa0d75934ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598cbe6a479c4d3a884a08605c878d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the largest country in the world and the second largest nation in Europe after China,'\n",
      "Generation 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa01a6d562e64f0cab83100dbb025333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca067bf364de4dfdaab36355c8bdaada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: \" a city of about 7 billion people, the capital of the world's second highest\"\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.9, top_k=10)\n",
    "\n",
    "for i in range(0,4):\n",
    "  print(f\"Generation {i+1}\")\n",
    "  outputs = llm.generate(prompts, sampling_params)\n",
    "  for output in outputs:\n",
    "      prompt = output.prompt\n",
    "      generated_text = output.outputs[0].text\n",
    "      print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Printing probability for each token\n",
    "\n",
    "In the previous test we have seen how top_k can be use to restrict the model to choose tokens up to the configured `k` for the tokens with the highest probabilities. This time we will do the same, but we will print the probabilities.\n",
    "\n",
    "You will see that the tokens always get the same probabilities, but since temperature is 0.9 the outputs will differ. Try to run the cell multiple times and you will see same probabilities but different output. Try changing the temperature to 0 and you will see the same output every time, that output will match with the word with the highest probability for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2aa510eaea40f39f50f261835e7c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45298a7a5bd34f1db6f81ef1ddf9e4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' a very important country for the EU. If I had to describe how I feel'\n",
      "Probabilities:\n",
      "  Step 1:\n",
      "    Token: ' a' (ID: 10) | Logprob: -2.5755 | %: 7.61%\n",
      "    Token: ' the' (ID: 5) | Logprob: -2.4231 | %: 8.86%\n",
      "    Token: ' not' (ID: 45) | Logprob: -3.2669 | %: 3.81%\n",
      "    Token: ' in' (ID: 11) | Logprob: -3.4700 | %: 3.11%\n",
      "    Token: ' being' (ID: 145) | Logprob: -3.8333 | %: 2.16%\n",
      "    Token: ' one' (ID: 65) | Logprob: -4.2942 | %: 1.36%\n",
      "    Token: ' home' (ID: 184) | Logprob: -4.2981 | %: 1.36%\n",
      "    Token: ' now' (ID: 122) | Logprob: -4.3489 | %: 1.29%\n",
      "    Token: ' getting' (ID: 562) | Logprob: -4.3919 | %: 1.24%\n",
      "    Token: ' going' (ID: 164) | Logprob: -4.4505 | %: 1.17%\n",
      "  Step 2:\n",
      "    Token: ' very' (ID: 182) | Logprob: -3.9039 | %: 2.02%\n",
      "    Token: ' city' (ID: 343) | Logprob: -2.7789 | %: 6.21%\n",
      "    Token: ' country' (ID: 247) | Logprob: -3.1618 | %: 4.24%\n",
      "    Token: ' capital' (ID: 812) | Logprob: -3.9860 | %: 1.86%\n",
      "    Token: ' place' (ID: 317) | Logprob: -4.0993 | %: 1.66%\n",
      "    Token: ' beautiful' (ID: 2721) | Logprob: -4.1539 | %: 1.57%\n",
      "    Token: ' great' (ID: 372) | Logprob: -4.3102 | %: 1.34%\n",
      "    Token: ' state' (ID: 194) | Logprob: -4.5875 | %: 1.02%\n",
      "    Token: ' nation' (ID: 1226) | Logprob: -4.6149 | %: 0.99%\n",
      "    Token: ' big' (ID: 380) | Logprob: -4.6188 | %: 0.99%\n",
      "  Step 3:\n",
      "    Token: ' important' (ID: 505) | Logprob: -2.7120 | %: 6.64%\n",
      "    Token: ' small' (ID: 650) | Logprob: -2.7276 | %: 6.54%\n",
      "    Token: ' beautiful' (ID: 2721) | Logprob: -3.3448 | %: 3.53%\n",
      "    Token: ' big' (ID: 380) | Logprob: -3.3839 | %: 3.39%\n",
      "    Token: ' good' (ID: 205) | Logprob: -3.3995 | %: 3.34%\n",
      "    Token: ' different' (ID: 430) | Logprob: -3.4073 | %: 3.31%\n",
      "    Token: ' dangerous' (ID: 2702) | Logprob: -3.4854 | %: 3.06%\n",
      "    Token: ' nice' (ID: 2579) | Logprob: -3.5870 | %: 2.77%\n",
      "    Token: ' large' (ID: 739) | Logprob: -3.8292 | %: 2.17%\n",
      "    Token: ' special' (ID: 780) | Logprob: -3.9424 | %: 1.94%\n",
      "  Step 4:\n",
      "    Token: ' country' (ID: 247) | Logprob: -2.4905 | %: 8.29%\n",
      "    Token: ' city' (ID: 343) | Logprob: -2.0530 | %: 12.84%\n",
      "    Token: ' place' (ID: 317) | Logprob: -2.2326 | %: 10.72%\n",
      "    Token: ' part' (ID: 233) | Logprob: -3.1506 | %: 4.28%\n",
      "    Token: ' and' (ID: 8) | Logprob: -3.2014 | %: 4.07%\n",
      "    Token: ' tourist' (ID: 8376) | Logprob: -3.9162 | %: 1.99%\n",
      "    Token: ' location' (ID: 2259) | Logprob: -3.9787 | %: 1.87%\n",
      "    Token: ' site' (ID: 1082) | Logprob: -4.0061 | %: 1.82%\n",
      "    Token: ' centre' (ID: 2100) | Logprob: -4.1545 | %: 1.57%\n",
      "    Token: ' market' (ID: 210) | Logprob: -4.2326 | %: 1.45%\n",
      "  Step 5:\n",
      "    Token: ' for' (ID: 13) | Logprob: -1.8123 | %: 16.33%\n",
      "    Token: '.' (ID: 4) | Logprob: -1.3982 | %: 24.70%\n",
      "    Token: ',' (ID: 6) | Logprob: -1.9920 | %: 13.64%\n",
      "    Token: ' to' (ID: 7) | Logprob: -2.1092 | %: 12.13%\n",
      "    Token: ' and' (ID: 8) | Logprob: -2.3514 | %: 9.52%\n",
      "    Token: ' in' (ID: 11) | Logprob: -2.5545 | %: 7.77%\n",
      "    Token: ' because' (ID: 142) | Logprob: -4.2107 | %: 1.48%\n",
      "    Token: ' but' (ID: 53) | Logprob: -4.4920 | %: 1.12%\n",
      "    Token: ' that' (ID: 14) | Logprob: -4.5154 | %: 1.09%\n",
      "    Token: ' with' (ID: 19) | Logprob: -4.5232 | %: 1.09%\n",
      "  Step 6:\n",
      "    Token: ' the' (ID: 5) | Logprob: -1.9454 | %: 14.29%\n",
      "    Token: ' many' (ID: 171) | Logprob: -2.5861 | %: 7.53%\n",
      "    Token: ' France' (ID: 1470) | Logprob: -2.9572 | %: 5.20%\n",
      "    Token: ' me' (ID: 162) | Logprob: -3.0431 | %: 4.77%\n",
      "    Token: ' us' (ID: 201) | Logprob: -3.2345 | %: 3.94%\n",
      "    Token: ' a' (ID: 10) | Logprob: -3.7892 | %: 2.26%\n",
      "    Token: ' French' (ID: 1515) | Logprob: -3.8165 | %: 2.20%\n",
      "    Token: ' its' (ID: 63) | Logprob: -4.1134 | %: 1.64%\n",
      "    Token: ' people' (ID: 82) | Logprob: -4.2032 | %: 1.49%\n",
      "    Token: ' European' (ID: 796) | Logprob: -4.3126 | %: 1.34%\n",
      "  Step 7:\n",
      "    Token: ' EU' (ID: 1281) | Logprob: -4.3249 | %: 1.32%\n",
      "    Token: ' French' (ID: 1515) | Logprob: -1.9890 | %: 13.68%\n",
      "    Token: ' development' (ID: 709) | Logprob: -3.1530 | %: 4.27%\n",
      "    Token: ' people' (ID: 82) | Logprob: -3.3718 | %: 3.43%\n",
      "    Token: ' world' (ID: 232) | Logprob: -3.5202 | %: 2.96%\n",
      "    Token: ' future' (ID: 499) | Logprob: -3.6413 | %: 2.62%\n",
      "    Token: ' country' (ID: 247) | Logprob: -3.7507 | %: 2.35%\n",
      "    Token: ' European' (ID: 796) | Logprob: -3.8093 | %: 2.22%\n",
      "    Token: ' rest' (ID: 1079) | Logprob: -4.1061 | %: 1.65%\n",
      "    Token: ' economy' (ID: 866) | Logprob: -4.1608 | %: 1.56%\n",
      "  Step 8:\n",
      "    Token: '.' (ID: 4) | Logprob: -0.8835 | %: 41.33%\n",
      "    Token: ',' (ID: 6) | Logprob: -1.6179 | %: 19.83%\n",
      "    Token: ' and' (ID: 8) | Logprob: -2.1022 | %: 12.22%\n",
      "    Token: ' but' (ID: 53) | Logprob: -3.9694 | %: 1.89%\n",
      "    Token: ' -' (ID: 111) | Logprob: -4.1413 | %: 1.59%\n",
      "    Token: ' because' (ID: 142) | Logprob: -4.2507 | %: 1.43%\n",
      "    Token: ' as' (ID: 25) | Logprob: -4.2663 | %: 1.40%\n",
      "    Token: ' (' (ID: 36) | Logprob: -4.3991 | %: 1.23%\n",
      "    Token: '\\n' (ID: 50118) | Logprob: -4.4460 | %: 1.17%\n",
      "    Token: '...' (ID: 734) | Logprob: -4.5085 | %: 1.10%\n",
      "  Step 9:\n",
      "    Token: ' If' (ID: 318) | Logprob: -3.5518 | %: 2.87%\n",
      "    Token: ' It' (ID: 85) | Logprob: -2.0440 | %: 12.95%\n",
      "    Token: ' ' (ID: 1437) | Logprob: -2.0674 | %: 12.65%\n",
      "    Token: '\\n' (ID: 50118) | Logprob: -2.1378 | %: 11.79%\n",
      "    Token: ' The' (ID: 20) | Logprob: -2.7315 | %: 6.51%\n",
      "    Token: ' I' (ID: 38) | Logprob: -3.0206 | %: 4.88%\n",
      "    Token: ' France' (ID: 1470) | Logprob: -3.5128 | %: 2.98%\n",
      "    Token: ' We' (ID: 166) | Logprob: -3.5596 | %: 2.84%\n",
      "    Token: ' But' (ID: 125) | Logprob: -3.7237 | %: 2.41%\n",
      "    Token: ' And' (ID: 178) | Logprob: -3.8878 | %: 2.05%\n",
      "  Step 10:\n",
      "    Token: ' I' (ID: 38) | Logprob: -3.5302 | %: 2.93%\n",
      "    Token: ' you' (ID: 47) | Logprob: -1.3153 | %: 26.84%\n",
      "    Token: ' the' (ID: 5) | Logprob: -2.2216 | %: 10.84%\n",
      "    Token: ' we' (ID: 52) | Logprob: -2.2450 | %: 10.59%\n",
      "    Token: ' they' (ID: 51) | Logprob: -2.3622 | %: 9.42%\n",
      "    Token: ' it' (ID: 24) | Logprob: -2.7997 | %: 6.08%\n",
      "    Token: ' France' (ID: 1470) | Logprob: -3.1395 | %: 4.33%\n",
      "    Token: ' there' (ID: 89) | Logprob: -3.4794 | %: 3.08%\n",
      "    Token: ' anything' (ID: 932) | Logprob: -4.0731 | %: 1.70%\n",
      "    Token: ' a' (ID: 10) | Logprob: -4.0966 | %: 1.66%\n",
      "  Step 11:\n",
      "    Token: ' had' (ID: 56) | Logprob: -2.1687 | %: 11.43%\n",
      "    Token: ' were' (ID: 58) | Logprob: -1.9421 | %: 14.34%\n",
      "    Token: ' was' (ID: 21) | Logprob: -2.1843 | %: 11.26%\n",
      "    Token: \"'m\" (ID: 437) | Logprob: -2.5984 | %: 7.44%\n",
      "    Token: ' am' (ID: 524) | Logprob: -2.9577 | %: 5.19%\n",
      "    Token: ' remember' (ID: 2145) | Logprob: -3.3406 | %: 3.54%\n",
      "    Token: ' lived' (ID: 3033) | Logprob: -3.6452 | %: 2.61%\n",
      "    Token: ' have' (ID: 33) | Logprob: -3.6687 | %: 2.55%\n",
      "    Token: ' could' (ID: 115) | Logprob: -3.8796 | %: 2.07%\n",
      "    Token: ' wanted' (ID: 770) | Logprob: -3.8796 | %: 2.07%\n",
      "  Step 12:\n",
      "    Token: ' to' (ID: 7) | Logprob: -0.4712 | %: 62.43%\n",
      "    Token: ' a' (ID: 10) | Logprob: -2.3462 | %: 9.57%\n",
      "    Token: ' the' (ID: 5) | Logprob: -2.7212 | %: 6.58%\n",
      "    Token: ' money' (ID: 418) | Logprob: -3.9790 | %: 1.87%\n",
      "    Token: ' my' (ID: 127) | Logprob: -4.2055 | %: 1.49%\n",
      "    Token: ' been' (ID: 57) | Logprob: -4.3540 | %: 1.29%\n",
      "    Token: ' any' (ID: 143) | Logprob: -4.6196 | %: 0.99%\n",
      "    Token: ' one' (ID: 65) | Logprob: -4.6235 | %: 0.98%\n",
      "    Token: ' an' (ID: 41) | Logprob: -4.7251 | %: 0.89%\n",
      "    Token: ' lived' (ID: 3033) | Logprob: -5.1001 | %: 0.61%\n",
      "  Step 13:\n",
      "    Token: ' describe' (ID: 6190) | Logprob: -4.4339 | %: 1.19%\n",
      "    Token: ' guess' (ID: 4443) | Logprob: -1.2621 | %: 28.31%\n",
      "    Token: ' choose' (ID: 2807) | Logprob: -1.4105 | %: 24.40%\n",
      "    Token: ' pick' (ID: 1339) | Logprob: -2.6293 | %: 7.21%\n",
      "    Token: ' make' (ID: 146) | Logprob: -3.9652 | %: 1.90%\n",
      "    Token: ' go' (ID: 213) | Logprob: -4.0043 | %: 1.82%\n",
      "    Token: ' say' (ID: 224) | Logprob: -4.1683 | %: 1.55%\n",
      "    Token: ' bet' (ID: 5673) | Logprob: -4.1761 | %: 1.54%\n",
      "    Token: ' give' (ID: 492) | Logprob: -4.3558 | %: 1.28%\n",
      "    Token: ' put' (ID: 342) | Logprob: -4.3949 | %: 1.23%\n",
      "  Step 14:\n",
      "    Token: ' how' (ID: 141) | Logprob: -3.8952 | %: 2.03%\n",
      "    Token: ' it' (ID: 24) | Logprob: -0.7976 | %: 45.04%\n",
      "    Token: ' the' (ID: 5) | Logprob: -1.8210 | %: 16.19%\n",
      "    Token: ' my' (ID: 127) | Logprob: -2.9694 | %: 5.13%\n",
      "    Token: ' a' (ID: 10) | Logprob: -3.1569 | %: 4.26%\n",
      "    Token: ' what' (ID: 99) | Logprob: -3.3522 | %: 3.50%\n",
      "    Token: ' this' (ID: 42) | Logprob: -3.5319 | %: 2.92%\n",
      "    Token: ' France' (ID: 1470) | Logprob: -3.5944 | %: 2.75%\n",
      "    Token: ' one' (ID: 65) | Logprob: -4.2858 | %: 1.38%\n",
      "    Token: ' any' (ID: 143) | Logprob: -4.4421 | %: 1.18%\n",
      "  Step 15:\n",
      "    Token: ' I' (ID: 38) | Logprob: -1.8506 | %: 15.71%\n",
      "    Token: ' to' (ID: 7) | Logprob: -2.0459 | %: 12.93%\n",
      "    Token: ' it' (ID: 24) | Logprob: -2.5146 | %: 8.09%\n",
      "    Token: ' much' (ID: 203) | Logprob: -2.5303 | %: 7.96%\n",
      "    Token: ' important' (ID: 505) | Logprob: -2.7568 | %: 6.35%\n",
      "    Token: ' the' (ID: 5) | Logprob: -3.0459 | %: 4.76%\n",
      "    Token: ' France' (ID: 1470) | Logprob: -3.7139 | %: 2.44%\n",
      "    Token: ' a' (ID: 10) | Logprob: -4.0615 | %: 1.72%\n",
      "    Token: ' great' (ID: 372) | Logprob: -4.2685 | %: 1.40%\n",
      "    Token: ' many' (ID: 171) | Logprob: -4.2920 | %: 1.37%\n",
      "  Step 16:\n",
      "    Token: ' feel' (ID: 619) | Logprob: -0.6806 | %: 50.63%\n",
      "    Token: ' felt' (ID: 1299) | Logprob: -2.0010 | %: 13.52%\n",
      "    Token: ' would' (ID: 74) | Logprob: -2.4150 | %: 8.94%\n",
      "    Token: ' see' (ID: 192) | Logprob: -3.4619 | %: 3.14%\n",
      "    Token: \"'d\" (ID: 1017) | Logprob: -3.8681 | %: 2.09%\n",
      "    Token: ' think' (ID: 206) | Logprob: -3.9619 | %: 1.90%\n",
      "    Token: ' want' (ID: 236) | Logprob: -4.1181 | %: 1.63%\n",
      "    Token: ' live' (ID: 697) | Logprob: -4.2510 | %: 1.43%\n",
      "    Token: ' wish' (ID: 2813) | Logprob: -4.4775 | %: 1.14%\n",
      "    Token: ' saw' (ID: 794) | Logprob: -5.1416 | %: 0.58%\n"
     ]
    }
   ],
   "source": [
    "# logprobs=-1 will return probabilities for every token, since we only care about the 10 highest probability tokens, we set this parameter to 10, otherwise we get probabilities for the whole vocabsize of the model\n",
    "sampling_params = SamplingParams(temperature=0.9, top_k=10, logprobs=10)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\n",
    "    import math\n",
    "    # The logprobs are a list of dictionaries (one dict per generated token)\n",
    "    # output.outputs[0].logprobs[i] corresponds to the i-th token generated\n",
    "    token_logprobs = output.outputs[0].logprobs\n",
    "    print(f\"Probabilities:\")\n",
    "    for i, top_k_dict in enumerate(token_logprobs):\n",
    "      # We can get the specific token that was sampled\n",
    "      # Note: In newer vLLM versions, the dict keys are token IDs (integers)\n",
    "      print(f\"  Step {i+1}:\")\n",
    "      for token_id, logprob_obj in top_k_dict.items():\n",
    "        # logprob_obj contains 'logprob', 'rank', and 'decoded_token' (if available)\n",
    "        print(f\"    Token: {logprob_obj.decoded_token!r} (ID: {token_id}) | Logprob: {logprob_obj.logprob:.4f} | %: {math.exp(logprob_obj.logprob)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Temperature 0, top_p 0.9\n",
    "\n",
    "In this test we will configure top_p to return tokens until the cumulative probability reaches the 0.9 (~90%). This means that the ammount of different tokens for the model to use we will get in each step will depend on the cumulative probability for all of them. We may get steps with lots of tokens to choose from, and other times we will get just a few. This will be easier to understand when we output the probabilities later.\n",
    "\n",
    "At the same time we're using the temperature=0, this means that from the list of words we will continue to choose the one with the highest probability. If we run the generation below 5 times, we will see the same output.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c486f6590674116aa28a8d1286f86b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f491fb25cc5944a0b7858e0aea97642c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n",
      "Generation 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77378d6a7918475680b35eb049b068e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2935949c06584799a1f16afd34788885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n",
      "Generation 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29f19974c6145ccbe635ccdf395b22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1170c549d041f88ccb38b1f9f9242c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n",
      "Generation 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5727e1d409234ccea0b897bf9a7a13f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0a8e66d8b043369b8a42e662962576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n",
      "Generation 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5051a66f72640daa6cba86e9208769b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9f7edae9e4480f9728cae4b38407d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0, top_p=0.9)\n",
    "\n",
    "for i in range(0,5):\n",
    "  print(f\"Generation {i+1}\")\n",
    "  outputs = llm.generate(prompts, sampling_params)\n",
    "  for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Temperature 0.9, top_p 0.9\n",
    "\n",
    "Same test as above, but this time we're setting temperature to 0.9. This means we will get different outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb01fee8a88e4e06b0692cd3efff11ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bb012443284854b79c6eba77c3cf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: \" just outside of Paris and you'll find many beautiful places in both city. If\"\n",
      "Generation 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54ce5304d2048779ee00fdf652e2905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdc4ac027544d76aab463739b665caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' seen as the highest concentrations of terrorism victims in the world, according to figures published'\n",
      "Generation 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e10e05575a546d8bf88a166650d53d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1574a5661944fba4be2fc780ff7f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' awash with laddies. From Paris to Bragat, and all'\n",
      "Generation 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dbb548824d4acf8c7ecd50f14e7518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606e89d2c42f4a568c6306a42cded9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the city in the heart of Paris. It is one of the biggest, most'\n",
      "Generation 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11da63dc10c4c8e94084e9cfe9179f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b775e18f58594dd6ab5bac5d4e45eb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: \" not one of the best states in the world. I don't think the French\"\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.9, top_p=0.9)\n",
    "\n",
    "for i in range(0,5):\n",
    "  print(f\"Generation {i+1}\")\n",
    "  outputs = llm.generate(prompts, sampling_params)\n",
    "  for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Printing probability for each token and cumulative probability\n",
    "\n",
    "We will see the two parameters that are taking place for token selection. In one side we will see how the list of tokens in each step is generated until we reach the top_p threshold, and we will see how from that list we get the token with the highest probability (temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a24b16419f4805a4f62efb5cab0697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7432dccf3884f53950e4d46ffb05c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n",
      "  Step 1:\n",
      "    Token: ' the'       | Prob:  8.86% | Cumul:  8.86%\n",
      "    Token: ' a'         | Prob:  7.61% | Cumul: 16.48%\n",
      "    Token: ' not'       | Prob:  3.81% | Cumul: 20.29%\n",
      "    Token: ' in'        | Prob:  3.11% | Cumul: 23.40%\n",
      "    Token: ' being'     | Prob:  2.16% | Cumul: 25.56%\n",
      "    Token: ' one'       | Prob:  1.36% | Cumul: 26.93%\n",
      "    Token: ' home'      | Prob:  1.36% | Cumul: 28.29%\n",
      "    Token: ' now'       | Prob:  1.29% | Cumul: 29.58%\n",
      "    Token: ' getting'   | Prob:  1.24% | Cumul: 30.82%\n",
      "    Top_p threshold reached (30.82%/30.0%). Skipping remaining vocab\n",
      "  Step 2:\n",
      "    Token: ' capital'   | Prob: 38.30% | Cumul: 38.30%\n",
      "    Top_p threshold reached (38.30%/30.0%). Skipping remaining vocab\n",
      "  Step 3:\n",
      "    Token: ' of'        | Prob: 89.68% | Cumul: 89.68%\n",
      "    Top_p threshold reached (89.68%/30.0%). Skipping remaining vocab\n",
      "  Step 4:\n",
      "    Token: ' the'       | Prob: 46.57% | Cumul: 46.57%\n",
      "    Top_p threshold reached (46.57%/30.0%). Skipping remaining vocab\n",
      "  Step 5:\n",
      "    Token: ' French'    | Prob: 10.51% | Cumul: 10.51%\n",
      "    Token: ' world'     | Prob:  6.47% | Cumul: 16.98%\n",
      "    Token: ' European'  | Prob:  6.13% | Cumul: 23.11%\n",
      "    Token: ' United'    | Prob:  5.04% | Cumul: 28.15%\n",
      "    Token: ' Republic'  | Prob:  4.52% | Cumul: 32.67%\n",
      "    Top_p threshold reached (32.67%/30.0%). Skipping remaining vocab\n",
      "  Step 6:\n",
      "    Token: ' Republic'  | Prob: 36.91% | Cumul: 36.91%\n",
      "    Top_p threshold reached (36.91%/30.0%). Skipping remaining vocab\n",
      "  Step 7:\n",
      "    Token: '.'          | Prob: 48.15% | Cumul: 48.15%\n",
      "    Top_p threshold reached (48.15%/30.0%). Skipping remaining vocab\n",
      "  Step 8:\n",
      "    Token: '\\n'         | Prob: 30.06% | Cumul: 30.06%\n",
      "    Top_p threshold reached (30.06%/30.0%). Skipping remaining vocab\n",
      "  Step 9:\n",
      "    Token: '\\n'         | Prob: 10.33% | Cumul: 10.33%\n",
      "    Token: 'I'          | Prob:  6.07% | Cumul: 16.40%\n",
      "    Token: 'The'        | Prob:  4.88% | Cumul: 21.28%\n",
      "    Token: 'It'         | Prob:  3.89% | Cumul: 25.17%\n",
      "    Token: 'And'        | Prob:  3.43% | Cumul: 28.60%\n",
      "    Token: 'That'       | Prob:  2.47% | Cumul: 31.07%\n",
      "    Top_p threshold reached (31.07%/30.0%). Skipping remaining vocab\n",
      "  Step 10:\n",
      "    Token: 'The'        | Prob: 18.07% | Cumul: 18.07%\n",
      "    Token: 'France'     | Prob:  5.78% | Cumul: 23.85%\n",
      "    Token: 'It'         | Prob:  5.38% | Cumul: 29.23%\n",
      "    Token: 'In'         | Prob:  3.70% | Cumul: 32.94%\n",
      "    Top_p threshold reached (32.94%/30.0%). Skipping remaining vocab\n",
      "  Step 11:\n",
      "    Token: ' capital'   | Prob: 52.34% | Cumul: 52.34%\n",
      "    Top_p threshold reached (52.34%/30.0%). Skipping remaining vocab\n",
      "  Step 12:\n",
      "    Token: ' of'        | Prob: 84.68% | Cumul: 84.68%\n",
      "    Top_p threshold reached (84.68%/30.0%). Skipping remaining vocab\n",
      "  Step 13:\n",
      "    Token: ' France'    | Prob: 71.52% | Cumul: 71.52%\n",
      "    Top_p threshold reached (71.52%/30.0%). Skipping remaining vocab\n",
      "  Step 14:\n",
      "    Token: ' is'        | Prob: 91.16% | Cumul: 91.16%\n",
      "    Top_p threshold reached (91.16%/30.0%). Skipping remaining vocab\n",
      "  Step 15:\n",
      "    Token: ' the'       | Prob: 76.87% | Cumul: 76.87%\n",
      "    Top_p threshold reached (76.87%/30.0%). Skipping remaining vocab\n",
      "  Step 16:\n",
      "    Token: ' capital'   | Prob: 88.57% | Cumul: 88.57%\n",
      "    Top_p threshold reached (88.57%/30.0%). Skipping remaining vocab\n"
     ]
    }
   ],
   "source": [
    "# logprobs=-1 will return probabilities for every token in the vocabsize of the model, we will then limit the output when we reach the top_p threshold\n",
    "sampling_params = SamplingParams(temperature=0, top_p=0.3, logprobs=-1)\n",
    "# We will use this to stop printing distribution/cumulative probabilities once we reached this threshold\n",
    "target_p = 0.3\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "  prompt = output.prompt\n",
    "  generated_text = output.outputs[0].text\n",
    "  print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "  token_logprobs_list = output.outputs[0].logprobs\n",
    "  for i, top_k_dict in enumerate(token_logprobs_list):\n",
    "    print(f\"  Step {i+1}:\")\n",
    "    # Sort by probability (High -> Low)\n",
    "    sorted_preds = sorted(\n",
    "      top_k_dict.values(),\n",
    "      key=lambda x: x.logprob,\n",
    "      reverse=True\n",
    "    )\n",
    "    current_cumulative = 0.0\n",
    "    for logprob_obj in sorted_preds:\n",
    "      p = math.exp(logprob_obj.logprob)\n",
    "      current_cumulative += p\n",
    "      print(f\"    Token: {logprob_obj.decoded_token!r:<12} | Prob: {p*100:5.2f}% | Cumul: {current_cumulative*100:5.2f}%\")\n",
    "\n",
    "      # We stop showing probabilities if we reach the threshold\n",
    "      if current_cumulative >= target_p:\n",
    "        print(f\"    Top_p threshold reached ({current_cumulative*100:5.2f}%/{target_p*100}%). Skipping remaining vocab\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
