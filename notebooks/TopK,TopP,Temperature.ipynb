{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Top_K, Top_P, and temperature for LLMs\n",
    "\n",
    "This notebook aims to describe what those parameters are and how changing them impacts text generation.\n",
    "\n",
    "Before describing them, we need to know about two ways LLMs can do sampling.\n",
    "\n",
    "- **Greedy sampling**: At each step, it chooses the token with the highest probability based on the preceding tokens.\n",
    "  - Pros:\n",
    "    - Produces output that is often highly coherent and reflects the most common patterns in the training data.\n",
    "    - Is deterministic, meaning the same prompt will always result in the same output.\n",
    "  - Cons:\n",
    "    - Can lead to repetitive or dull text because it never deviates from the most likely path.\n",
    "    - It's a short-sighted approach, as the best next word might not be part of the best overall sequence.\n",
    "\n",
    "\n",
    "- **Random sampling**: It randomly selects a token from a probability distribution. Tokens with higher probabilities are more likely to be chosen, but even low-probability tokens have a chance of being selected.\n",
    "  - Pros:\n",
    "    - Generates more diverse and creative output.\n",
    "    - Can help avoid the repetitive loops that greedy sampling is prone to.\n",
    "  - Cons:\n",
    "    - Can produce lower-quality or less coherent output compared to greedy sampling.\n",
    "    - Is non-deterministic; running it multiple times will likely yield different results.\n",
    "\n",
    "The way to control if a model does greedy or random sampling is influenced by the use of `top_k`, `top_p` and `temperature` parameters.\n",
    "\n",
    "## Top_K\n",
    "\n",
    "Limits the model's output to the top-k most probable tokens at each generation step.\n",
    "\n",
    "### Example\n",
    "\n",
    "For the prompt `The future of AI is`, with a top_k=5 this is what we will get you the following tokens:\n",
    "\n",
    "```\n",
    "Token: ' in' (ID: 11) | Logprob: -2.1674 | %: 11.45%\n",
    "Token: ' not' (ID: 45) | Logprob: -3.1830 | %: 4.15%\n",
    "Token: ' now' (ID: 122) | Logprob: -3.4174 | %: 3.28%\n",
    "Token: ' here' (ID: 259) | Logprob: -3.4330 | %: 3.23%\n",
    "Token: ' a' (ID: 10) | Logprob: -3.4955 | %: 3.03%\n",
    "```\n",
    "\n",
    "As you can see we got the list of the 5 most probable tokens.\n",
    "\n",
    "## Top_P\n",
    "\n",
    "Filters out tokens when the cumulative probability (top_p) is reached.\n",
    "Top_p will consider more tokens compared to using Top_k, this is because the model will include as much tokens as needed until the cumulative probabilities reach the specific top_p.\n",
    "\n",
    "Example:\n",
    "\n",
    "top_k=5 will get you the 5 tokens with the highest probability.\n",
    "top_p=0.3 will get you tokens until the cumulative probabilities for all of them reach 0.3 (30%) value. This means that if we have a list of tokens with a % probability each. We will sum them from highest to lowest % and we will stop when we reach the threshold.\n",
    "\n",
    "```\n",
    "Token: ' the'       | Prob:  8.86% | Cumul:  8.86%\n",
    "Token: ' a'         | Prob:  7.61% | Cumul: 16.48%\n",
    "Token: ' not'       | Prob:  3.81% | Cumul: 20.29%\n",
    "Token: ' in'        | Prob:  3.11% | Cumul: 23.40%\n",
    "Token: ' being'     | Prob:  2.16% | Cumul: 25.56%\n",
    "Token: ' one'       | Prob:  1.36% | Cumul: 26.93%\n",
    "Token: ' home'      | Prob:  1.36% | Cumul: 28.29%\n",
    "Token: ' now'       | Prob:  1.29% | Cumul: 29.58%\n",
    "Token: ' getting'   | Prob:  1.24% | Cumul: 30.82%\n",
    "Top_p threshold reached (30.82%/30.0%). Skipping remaining vocab\n",
    "```\n",
    "\n",
    "## Temperature\n",
    "\n",
    "Adjusts token prediction randomness by scaling the log probabilities. Higher temperatures lead to more creative outputs, while lower lead to more predictable responses.\n",
    "\n",
    "\n",
    "## vLLM\n",
    "\n",
    "We will be using vLLM python bindings, the parameters can be set via the [SamplingParams](https://docs.vllm.ai/en/stable/api/vllm/#vllm.SamplingParams) class. Below the descriptions for each parameter.\n",
    "\n",
    "- [temperature](https://docs.vllm.ai/en/stable/api/vllm/#vllm.SamplingParams.temperature) – Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling. (0, 1)\n",
    "\n",
    "- [top_p](https://docs.vllm.ai/en/stable/api/vllm/#vllm.SamplingParams.top_p) – Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1). Set to 1 to consider all tokens.\n",
    "\n",
    "- [top_k](https://docs.vllm.ai/en/stable/api/vllm/#vllm.SamplingParams.top_k) – Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv venv --python 3.12 --seed --clear\n",
    "!source .venv/bin/activate\n",
    "!uv pip install vllm --torch-backend=auto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# max_logprobs=-1 will help us see token distribution/cumulative probabilities later\n",
    "llm = LLM(model=\"facebook/opt-125m\", max_logprobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top_K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Temperature 0, top_k 10\n",
    "\n",
    "In this test we will configure top_k to return the top 10 tokens with the highest probability, on top of that we are configuring a temperature of 0, this means the model will always use the token with the highest probability. We will run 5 generations and we will see we always get the same output.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0, top_k=10)\n",
    "\n",
    "for i in range(0,4):\n",
    "  print(f\"Generation {i+1}\")\n",
    "  outputs = llm.generate(prompts, sampling_params)\n",
    "  for output in outputs:\n",
    "      prompt = output.prompt\n",
    "      generated_text = output.outputs[0].text\n",
    "      print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Temperature 0.9, top_k 10\n",
    "\n",
    "In this test we will configure top_k to return the top 10 tokens with the highest probability, but this time, temperature is 0.9, this should led to different generations. The model will use on of the top 10 tokens in a less predictable way. We will run 5 generations and we will likely see different outputs for each generation.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.9, top_k=10)\n",
    "\n",
    "for i in range(0,4):\n",
    "  print(f\"Generation {i+1}\")\n",
    "  outputs = llm.generate(prompts, sampling_params)\n",
    "  for output in outputs:\n",
    "      prompt = output.prompt\n",
    "      generated_text = output.outputs[0].text\n",
    "      print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Printing probability for each token\n",
    "\n",
    "In the previous test we have seen how top_k can be use to restrict the model to choose tokens up to the configured `k` for the tokens with the highest probabilities. This time we will do the same, but we will print the probabilities.\n",
    "\n",
    "You will see that the tokens always get the same probabilities, but since temperature is 0.9 the outputs will differ. Try to run the cell multiple times and you will see same probabilities but different output. Try changing the temperature to 0 and you will see the same output every time, that output will match with the word with the highest probability for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logprobs=-1 will return probabilities for every token, since we only care about the 10 highest probability tokens, we set this parameter to 10, otherwise we get probabilities for the whole vocabsize of the model\n",
    "sampling_params = SamplingParams(temperature=0.9, top_k=10, logprobs=10)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\n",
    "    import math\n",
    "    # The logprobs are a list of dictionaries (one dict per generated token)\n",
    "    # output.outputs[0].logprobs[i] corresponds to the i-th token generated\n",
    "    token_logprobs = output.outputs[0].logprobs\n",
    "    print(f\"Probabilities:\")\n",
    "    for i, top_k_dict in enumerate(token_logprobs):\n",
    "      # We can get the specific token that was sampled\n",
    "      # Note: In newer vLLM versions, the dict keys are token IDs (integers)\n",
    "      print(f\"  Step {i+1}:\")\n",
    "      for token_id, logprob_obj in top_k_dict.items():\n",
    "        # logprob_obj contains 'logprob', 'rank', and 'decoded_token' (if available)\n",
    "        print(f\"    Token: {logprob_obj.decoded_token!r} (ID: {token_id}) | Logprob: {logprob_obj.logprob:.4f} | %: {math.exp(logprob_obj.logprob)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Temperature 0, top_p 0.9\n",
    "\n",
    "In this test we will configure top_p to return tokens until the cumulative probability reaches the 0.9 (~90%). This means that the ammount of different tokens for the model to use we will get in each step will depend on the cumulative probability for all of them. We may get steps with lots of tokens to choose from, and other times we will get just a few. This will be easier to understand when we output the probabilities later.\n",
    "\n",
    "At the same time we're using the temperature=0, this means that from the list of words we will continue to choose the one with the highest probability. If we run the generation below 5 times, we will see the same output.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0, top_p=0.9)\n",
    "\n",
    "for i in range(0,5):\n",
    "  print(f\"Generation {i+1}\")\n",
    "  outputs = llm.generate(prompts, sampling_params)\n",
    "  for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Temperature 0.9, top_p 0.9\n",
    "\n",
    "Same test as above, but this time we're setting temperature to 0.9. This means we will get different outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.9, top_p=0.9)\n",
    "\n",
    "for i in range(0,5):\n",
    "  print(f\"Generation {i+1}\")\n",
    "  outputs = llm.generate(prompts, sampling_params)\n",
    "  for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Printing probability for each token and cumulative probability\n",
    "\n",
    "We will see the two parameters that are taking place for token selection. In one side we will see how the list of tokens in each step is generated until we reach the top_p threshold, and we will see how from that list we get the token with the highest probability (temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logprobs=-1 will return probabilities for every token in the vocabsize of the model, we will then limit the output when we reach the top_p threshold\n",
    "sampling_params = SamplingParams(temperature=0, top_p=0.3, logprobs=-1)\n",
    "# We will use this to stop printing distribution/cumulative probabilities once we reached this threshold\n",
    "target_p = 0.3\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "  prompt = output.prompt\n",
    "  generated_text = output.outputs[0].text\n",
    "  print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "  token_logprobs_list = output.outputs[0].logprobs\n",
    "  for i, top_k_dict in enumerate(token_logprobs_list):\n",
    "    print(f\"  Step {i+1}:\")\n",
    "    # Sort by probability (High -> Low)\n",
    "    sorted_preds = sorted(\n",
    "      top_k_dict.values(),\n",
    "      key=lambda x: x.logprob,\n",
    "      reverse=True\n",
    "    )\n",
    "    current_cumulative = 0.0\n",
    "    for logprob_obj in sorted_preds:\n",
    "      p = math.exp(logprob_obj.logprob)\n",
    "      current_cumulative += p\n",
    "      print(f\"    Token: {logprob_obj.decoded_token!r:<12} | Prob: {p*100:5.2f}% | Cumul: {current_cumulative*100:5.2f}%\")\n",
    "\n",
    "      # We stop showing probabilities if we reach the threshold\n",
    "      if current_cumulative >= target_p:\n",
    "        print(f\"    Top_p threshold reached ({current_cumulative*100:5.2f}%/{target_p*100}%). Skipping remaining vocab\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
